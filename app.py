#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…· - WebæœåŠ¡å™¨
"""

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import os
import sys
from translation_qa_tool import TranslationQA

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€QAå·¥å…·å®ä¾‹ï¼ˆå¤ç”¨ä»¥æé«˜æ€§èƒ½ï¼‰
qa_tool = None


def get_qa_tool():
    """è·å–æˆ–åˆå§‹åŒ–QAå·¥å…·å®ä¾‹"""
    global qa_tool
    if qa_tool is None:
        print("åˆå§‹åŒ–ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…·...")
        qa_tool = TranslationQA(
            similarity_threshold=0.7,
            max_align=6,
            top_k=5,
            skip=-1.0,
            win=10,
            auto_detect_language=True,  # å¯ç”¨è‡ªåŠ¨è¯­è¨€æ£€æµ‹ï¼ˆä½¿ç”¨fastTextï¼‰
            force_split_threshold=0.3,  # é™ä½é˜ˆå€¼ï¼ˆ0.5 -> 0.3ï¼‰ï¼Œé¿å…è¯¯æ‹†æ•£
            use_min_similarity=False    # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ï¼ˆæ›´å®½æ¾ï¼‰
        )
        print("âœ“ å·¥å…·åˆå§‹åŒ–å®Œæˆ")
    return qa_tool


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/check', methods=['POST'])
def check_translation():
    """
    ç¿»è¯‘è´¨é‡æ£€æŸ¥API

    è¯·æ±‚ä½“:
    {
        "source_text": "åŸæ–‡",
        "target_text": "è¯‘æ–‡",
        "similarity_threshold": 0.7,      // å¯é€‰ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼
        "force_split_threshold": 0.5,     // å¯é€‰ï¼Œå¼ºåˆ¶æ‹†æ•£é˜ˆå€¼
        "max_align": 5,                   // å¯é€‰ï¼Œæœ€å¤§å¯¹é½æ•°
        "top_k": 3,                       // å¯é€‰ï¼ŒTop-Kå‚æ•°
        "skip": -0.1,                     // å¯é€‰ï¼Œè·³è¿‡æƒ©ç½š
        "win": 5,                         // å¯é€‰ï¼Œçª—å£å¤§å°
        "score_threshold": 0.0,           // å¯é€‰ï¼Œåˆ†æ•°é˜ˆå€¼
        "use_min_similarity": true        // å¯é€‰ï¼Œä½¿ç”¨æœ€å°ç›¸ä¼¼åº¦
    }

    è¿”å›:
    {
        "success": true,
        "data": {
            "csv": "CSVæ ¼å¼çš„æŠ¥å‘Š",
            "summary": {...},
            "issues": {...}
        }
    }
    """
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'è¯·æ±‚æ•°æ®ä¸ºç©º'
            }), 400
        
        source_text = data.get('source_text', '').strip()
        target_text = data.get('target_text', '').strip()
        
        if not source_text or not target_text:
            return jsonify({
                'success': False,
                'error': 'åŸæ–‡å’Œè¯‘æ–‡ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # è·å–å¯é€‰å‚æ•°
        similarity_threshold = data.get('similarity_threshold', 0.7)
        force_split_threshold = data.get('force_split_threshold', 0.5)

        # è·å–é«˜çº§å‚æ•°
        max_align = data.get('max_align', 5)
        top_k = data.get('top_k', 3)
        skip = data.get('skip', -0.1)
        win = data.get('win', 5)
        score_threshold = data.get('score_threshold', 0.0)
        use_min_similarity = data.get('use_min_similarity', True)

        # æ›´æ–°å‚æ•°ï¼ˆå¦‚æœä¸å½“å‰ä¸åŒï¼‰
        tool = get_qa_tool()
        tool.similarity_threshold = similarity_threshold
        tool.force_split_threshold = force_split_threshold
        tool.max_align = max_align
        tool.top_k = top_k
        tool.skip = skip
        tool.win = win
        tool.score_threshold = score_threshold
        tool.use_min_similarity = use_min_similarity

        # æ‰§è¡Œæ£€æŸ¥ï¼ˆä½¿ç”¨ fastText è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼‰
        print(f"å¼€å§‹æ£€æŸ¥ç¿»è¯‘...")
        print(f"  ä½¿ç”¨ fastText è‡ªåŠ¨æ£€æµ‹è¯­è¨€...")

        results = tool.check_translation(
            source_text=source_text,
            target_text=target_text,
            source_language='auto',  # è‡ªåŠ¨æ£€æµ‹
            target_language='auto',  # è‡ªåŠ¨æ£€æµ‹
            is_split=False  # è®©å·¥å…·è‡ªåŠ¨åˆ†å¥
        )
        
        # ç”ŸæˆCSVæ ¼å¼çš„æŠ¥å‘Š
        csv_lines = []
        csv_lines.append("åŸæ–‡ (Source),è¯‘æ–‡ (Target),æºç´¢å¼•,ç›®æ ‡ç´¢å¼•,ç›¸ä¼¼åº¦ (Similarity),å¼‚å¸¸æƒ…å†µ (Exception)")
        
        # æ”¶é›†æ‰€æœ‰è¡Œ
        all_rows = []
        
        # è·å–è¢«æ‹†æ•£çš„å¯¹é½ç»„
        force_split_set = set()
        for fs_item in results.get('force_split_alignments', []):
            force_split_set.add((tuple(fs_item['src_indices']), tuple(fs_item['tgt_indices'])))
        
        # å¯¹é½ç»„
        for item in results['alignments']:
            src_indices = item['src_indices']
            tgt_indices = item['tgt_indices']

            # è·³è¿‡è¢«æ‹†æ•£çš„å¯¹é½ç»„
            if (tuple(src_indices), tuple(tgt_indices)) in force_split_set:
                continue

            similarity = item['similarity']

            # æ£€æŸ¥å¼‚å¸¸
            exception = "OK"
            if similarity is None:
                exception = "ç©ºå¯¹é½"
            elif similarity < similarity_threshold:
                exception = "ç›¸ä¼¼åº¦ä½ (Low Similarity)"

            # ğŸ”´ ä¿®å¤: ä½¿ç”¨ç¬¬ä¸€ä¸ªæºç´¢å¼•ä½œä¸ºæ’åºé”®ï¼ˆå¦‚æœæœ‰æºç´¢å¼•çš„è¯ï¼‰
            # å¯¹äºN:Må¯¹é½ï¼Œæ‰€æœ‰è¡Œéƒ½åº”è¯¥ä½¿ç”¨ç›¸åŒçš„æ’åºé”®ï¼Œè¿™æ ·å®ƒä»¬ä¼šè¢«æ’åœ¨ä¸€èµ·
            if len(src_indices) > 0:
                sort_key = src_indices[0]
            elif len(tgt_indices) > 0:
                # å¦‚æœæ²¡æœ‰æºç´¢å¼•ï¼ˆå¢æ·»ï¼‰ï¼Œä½¿ç”¨ç›®æ ‡ç´¢å¼• + å¤§åç§»é‡
                sort_key = 999999 + tgt_indices[0]
            else:
                sort_key = 999999

            # N:Må¯¹é½å±•å¼€
            max_len = max(len(src_indices), len(tgt_indices))
            for i in range(max_len):
                src_text = item['src_texts'][i] if i < len(item['src_texts']) else ""
                tgt_text = item['tgt_texts'][i] if i < len(item['tgt_texts']) else ""
                src_idx = src_indices[i] if i < len(src_indices) else ""
                tgt_idx = tgt_indices[i] if i < len(tgt_indices) else ""

                # æ¯ä¸€è¡Œéƒ½æ˜¾ç¤ºç›¸ä¼¼åº¦å’Œå¼‚å¸¸ï¼ˆN:Må¯¹é½çš„æ‰€æœ‰è¡Œå…±äº«åŒä¸€ä¸ªç›¸ä¼¼åº¦ï¼‰
                sim_str = f"{similarity:.4f}" if similarity is not None else ""
                exc_str = exception

                # ğŸ”´ ä¿®å¤: ä¸ºäº†ä¿æŒN:Må¯¹é½çš„å¤šè¡Œåœ¨ä¸€èµ·ï¼Œä½¿ç”¨å­æ’åºé”®
                # sort_keyç›¸åŒæ—¶ï¼ŒæŒ‰iæ’åº
                subsort_key = sort_key + (i * 0.001)  # æ·»åŠ å°æ•°éƒ¨åˆ†æ¥ä¿æŒé¡ºåº

                all_rows.append({
                    'src_text': src_text,
                    'tgt_text': tgt_text,
                    'src_index': src_idx,
                    'tgt_index': tgt_idx,
                    'similarity': sim_str,
                    'exception': exc_str,
                    '_sort_key': subsort_key  # ä½¿ç”¨å­æ’åºé”®
                })
        
        # ç¼ºå¤±å’Œå¢æ·»
        for item in results['issues']['omissions']:
            all_rows.append({
                'src_text': item['src_text'],
                'tgt_text': "",
                'src_index': item['src_index'],
                'tgt_index': "",
                'similarity': "",
                'exception': "ç¼ºå¤± (Omission)",
                '_sort_key': item['src_index']
            })
        
        for item in results['issues']['additions']:
            all_rows.append({
                'src_text': "",
                'tgt_text': item['tgt_text'],
                'src_index': "",
                'tgt_index': item['tgt_index'],
                'similarity': "",
                'exception': "å¢æ·» (Addition)",
                '_sort_key': 999999
            })

        # æŒ‰æºç´¢å¼•æ’åº
        all_rows.sort(key=lambda x: x['_sort_key'])

        # ç”ŸæˆCSV
        for row in all_rows:
            csv_lines.append(f'"{row["src_text"]}","{row["tgt_text"]}",{row["src_index"]},{row["tgt_index"]},{row["similarity"]},{row["exception"]}')

        csv_content = "\n".join(csv_lines)

        # è¿”å›ç»“æœ
        return jsonify({
            'success': True,
            'data': {
                'csv': csv_content,
                'summary': {
                    # å‰ç«¯éœ€è¦çš„å­—æ®µ
                    'src_count': results['metadata']['source_sentences'],
                    'tgt_count': results['metadata']['target_sentences'],
                    'alignment_count': results['metadata']['alignments'],
                    'similarity_threshold': results['metadata']['similarity_threshold'],
                    # åŸæœ‰çš„ç»Ÿè®¡å­—æ®µ
                    'total_issues': results['summary']['total_issues'],
                    'omission_count': results['summary']['omission_count'],
                    'addition_count': results['summary']['addition_count'],
                    'low_similarity_count': results['summary']['low_similarity_count'],
                    'force_split_count': results['summary']['force_split_count']
                },
                'issues': {
                    'omissions': results['issues']['omissions'],
                    'additions': results['issues']['additions'],
                    'low_similarity': results['issues']['low_similarity']
                },
                'force_split_count': len(results.get('force_split_alignments', []))
            }
        })

    except Exception as e:
        import traceback
        error_msg = str(e)
        traceback_msg = traceback.format_exc()
        print(f"é”™è¯¯: {error_msg}")
        print(traceback_msg)

        return jsonify({
            'success': False,
            'error': error_msg,
            'traceback': traceback_msg
        }), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'ok',
        'model_loaded': qa_tool is not None
    })


if __name__ == '__main__':
    print("="*80)
    print("ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…· - WebæœåŠ¡å™¨")
    print("="*80)
    print("\næ­£åœ¨å¯åŠ¨æœåŠ¡å™¨...")
    print("è®¿é—®åœ°å€: http://localhost:5001")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")

    app.run(host='0.0.0.0', port=5001, debug=True)

