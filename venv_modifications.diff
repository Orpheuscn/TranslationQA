=== encoder.py å®Œæ•´diff ===
--- /tmp/bertalign_original/bertalign/encoder.py 2025-12-21 22:26:00
+++ venv/lib/python3.12/site-packages/bertalign/encoder.py 2025-12-19 21:04:44
@@ -1,24 +1,77 @@
 import numpy as np
+import os
 
-from sentence_transformers import SentenceTransformer
+# ä¿®è¡¥: ä½¿ç”¨ONNXç‰ˆæœ¬é¿å…macOS ARM64ä¸Šçš„SentenceTransformerå´©æºƒ
+USE_ONNX = True
+
+if USE_ONNX:
+    import onnxruntime as ort
+    from transformers import AutoTokenizer
+else:
+    from sentence_transformers import SentenceTransformer
+
 from bertalign.utils import yield_overlaps
 
 class Encoder:
     def __init__(self, model_name):
-        self.model = SentenceTransformer(model_name)
         self.model_name = model_name
 
+        if USE_ONNX:
+            # ä½¿ç”¨ONNXç‰ˆæœ¬çš„LaBSE
+            model_path = os.path.join(os.getcwd(), "labse_onnx")
+            if not os.path.exists(model_path):
+                raise FileNotFoundError(f"ONNXæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
+
+            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
+            onnx_model_path = os.path.join(model_path, "model.onnx")
+            self.session = ort.InferenceSession(onnx_model_path)
+            self.model = None
+            print(f"âœ“ ä½¿ç”¨ONNXç‰ˆæœ¬çš„LaBSE (é¿å…macOS ARM64å´©æºƒ)")
+        else:
+            self.model = SentenceTransformer(model_name)
+            self.tokenizer = None
+            self.session = None
+
+    def encode_onnx(self, sentences):
+        """ä½¿ç”¨ONNXæ¨¡å‹ç¼–ç å¥å­"""
+        inputs = self.tokenizer(
+            sentences,
+            padding=True,
+            truncation=True,
+            max_length=512,
+            return_tensors="np"
+        )
+
+        onnx_inputs = {
+            "input_ids": inputs["input_ids"].astype(np.int64),
+            "attention_mask": inputs["attention_mask"].astype(np.int64),
+            "token_type_ids": inputs["token_type_ids"].astype(np.int64),
+        }
+
+        outputs = self.session.run(None, onnx_inputs)
+        embeddings = outputs[0][:, 0, :].astype(np.float32)
+
+        # L2å½’ä¸€åŒ–
+        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
+        embeddings = embeddings / norms
+
+        return embeddings
+
     def transform(self, sents, num_overlaps):
         overlaps = []
         for line in yield_overlaps(sents, num_overlaps):
             overlaps.append(line)
 
-        sent_vecs = self.model.encode(overlaps)
+        if USE_ONNX:
+            sent_vecs = self.encode_onnx(overlaps)
+        else:
+            sent_vecs = self.model.encode(overlaps)
+
         embedding_dim = sent_vecs.size // (len(sents) * num_overlaps)
-        sent_vecs.resize(num_overlaps, len(sents), embedding_dim)
+        sent_vecs = sent_vecs.reshape(num_overlaps, len(sents), embedding_dim)
 
         len_vecs = [len(line.encode("utf-8")) for line in overlaps]
         len_vecs = np.array(len_vecs)
-        len_vecs.resize(num_overlaps, len(sents))
+        len_vecs = len_vecs.reshape(num_overlaps, len(sents))
 
         return sent_vecs, len_vecs

=== aligner.py å®Œæ•´diff ===
--- /tmp/bertalign_original/bertalign/aligner.py 2025-12-21 22:26:00
+++ venv/lib/python3.12/site-packages/bertalign/aligner.py 2025-12-21 22:05:22
@@ -15,19 +15,25 @@
                  margin=True,
                  len_penalty=True,
                  is_split=False,
+                 src_lang=None,  # ğŸ†• å¯é€‰ï¼šæºè¯­è¨€ä»£ç ï¼ˆé¿å…è°ƒç”¨Google Translateï¼‰
+                 tgt_lang=None,  # ğŸ†• å¯é€‰ï¼šç›®æ ‡è¯­è¨€ä»£ç ï¼ˆé¿å…è°ƒç”¨Google Translateï¼‰
                ):
-        
+
         self.max_align = max_align
         self.top_k = top_k
         self.win = win
         self.skip = skip
         self.margin = margin
         self.len_penalty = len_penalty
-        
+
         src = clean_text(src)
         tgt = clean_text(tgt)
-        src_lang = detect_lang(src)
-        tgt_lang = detect_lang(tgt)
+
+        # ğŸ†• å¦‚æœæä¾›äº†è¯­è¨€ä»£ç ï¼Œä½¿ç”¨æä¾›çš„ï¼›å¦åˆ™è°ƒç”¨ Google Translate æ£€æµ‹
+        if src_lang is None:
+            src_lang = detect_lang(src)
+        if tgt_lang is None:
+            tgt_lang = detect_lang(tgt)
         
         if is_split:
             src_sents = src.splitlines()

=== corelib.py å®Œæ•´diff ===
--- /tmp/bertalign_original/bertalign/corelib.py 2025-12-21 22:26:00
+++ venv/lib/python3.12/site-packages/bertalign/corelib.py 2025-12-19 21:05:27
@@ -388,15 +388,29 @@
         D: numpy array. Similarity score matrix of shape (num_src_sents, k).
         I: numpy array. Target index matrix of shape (num_src_sents, k).
     """
+    n_src = src_vecs.shape[0]
+    n_tgt = tgt_vecs.shape[0]
     embedding_size = src_vecs.shape[1]
+    k = min(k, n_tgt)
+
     if torch.cuda.is_available() and platform == 'linux': # GPU version
-        res = faiss.StandardGpuResources() 
+        res = faiss.StandardGpuResources()
         index = faiss.IndexFlatIP(embedding_size)
         gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
-        gpu_index.add(tgt_vecs) 
+        gpu_index.add(tgt_vecs)
         D, I = gpu_index.search(src_vecs, k)
-    else: # CPU version
+    else: # CPU version - ä¿®å¤macOS ARM64æ‰¹é‡æœç´¢bug
         index = faiss.IndexFlatIP(embedding_size)
         index.add(tgt_vecs)
-        D, I = index.search(src_vecs, k)
+
+        # Workaround: é€ä¸ªæœç´¢é¿å…macOS ARM64ä¸Šçš„FAISSæ‰¹é‡æœç´¢bug
+        D = np.zeros((n_src, k), dtype=np.float32)
+        I = np.zeros((n_src, k), dtype=np.int64)
+
+        for i in range(n_src):
+            query = src_vecs[i:i+1, :]
+            d, idx = index.search(query, k)
+            D[i] = d[0]
+            I[i] = idx[0]
+
     return D, I

=== FastText.py å®Œæ•´diff ===
--- /tmp/fasttext_original/python/fasttext_module/fasttext/FastText.py 2025-12-21 22:42:58
+++ venv/lib/python3.12/site-packages/fasttext/FastText.py 2025-12-21 20:09:14
@@ -236,7 +236,7 @@
             else:
                 probs, labels = ([], ())
 
-            return labels, np.array(probs, copy=False)
+            return labels, np.asarray(probs)
 
     def get_input_matrix(self):
         """
