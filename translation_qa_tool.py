#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…· (Translation Quality Assurance Tool)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨Bertalignè¿›è¡ŒN:Må¥å­å¯¹é½
2. ä½¿ç”¨LaBSE ONNXè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
3. æ£€æµ‹ä¸‰ç§ç¿»è¯‘å¼‚å¸¸ï¼š
   - ç¼ºå¤± (Omission): åŸæ–‡å¥å­åœ¨è¯‘æ–‡ä¸­æ²¡æœ‰å¯¹åº”
   - å¢æ·» (Addition): è¯‘æ–‡å¥å­åœ¨åŸæ–‡ä¸­æ²¡æœ‰å¯¹åº”
   - ç›¸ä¼¼åº¦ä½/è¯­ä¹‰æ­ªæ›² (Low Similarity): å¯¹é½ç»„ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼
"""

import numpy as np
import json
import pandas as pd
from datetime import datetime
from bertalign import Bertalign
from labse_onnx_encoder import LaBSEOnnxEncoder
from text_splitter import TextSplitter
from model_config import setup_hanlp_env

# è®¾ç½® HanLP ç¯å¢ƒå˜é‡ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
setup_hanlp_env()


class TranslationQA:
    """ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…·"""

    def __init__(self, similarity_threshold=0.7, max_align=6, top_k=5, score_threshold=0.15,
                 skip=-1.0, win=10, auto_detect_language=True,
                 force_split_threshold=0.5, use_min_similarity=True, auto_split_nm=False):
        """
        åˆå§‹åŒ–ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…·

        å‚æ•°:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºè¯­ä¹‰æ­ªæ›²
            max_align: Bertalignçš„æœ€å¤§å¯¹é½æ•° (N:Mä¸­çš„max(N,M))
            top_k: Bertalignçš„top-kå‚æ•°
            score_threshold: Bertalignçš„åˆ†æ•°é˜ˆå€¼
            skip: Bertalignçš„è·³è¿‡æƒ©ç½šï¼ˆè´Ÿæ•°è¶Šå¤§ï¼Œè¶Šå€¾å‘äºN:Må¯¹é½è€Œéç¼ºå¤±/å¢æ·»ï¼‰
            win: Bertalignçš„çª—å£å¤§å°
            auto_detect_language: æ˜¯å¦è‡ªåŠ¨æ£€æµ‹è¯­è¨€ï¼ˆä½¿ç”¨ fastTextï¼‰
            force_split_threshold: å¼ºåˆ¶æ‹†æ•£é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„å¯¹é½ç»„å°†è¢«æ‹†æ•£ä¸ºç¼ºå¤±+å¢æ·» (é»˜è®¤0.5)
            use_min_similarity: N:Må¯¹é½æ—¶ä½¿ç”¨æœ€å°ç›¸ä¼¼åº¦è€Œéå¹³å‡ç›¸ä¼¼åº¦ (é»˜è®¤Trueï¼Œæ›´ä¸¥æ ¼)
            auto_split_nm: è‡ªåŠ¨æ‹†æ•£N:Må¯¹é½ä¸ºå¤šä¸ª1:1å¯¹é½ï¼ˆå¦‚æœN==Mä¸”æ‹†æ•£åç›¸ä¼¼åº¦æ›´é«˜ï¼‰(é»˜è®¤False)
        """
        self.similarity_threshold = similarity_threshold
        self.max_align = max_align
        self.top_k = top_k
        self.score_threshold = score_threshold
        self.skip = skip
        self.win = win
        self.force_split_threshold = force_split_threshold
        self.auto_split_nm = auto_split_nm
        self.use_min_similarity = use_min_similarity

        # åˆå§‹åŒ–ç¼–ç å™¨ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        self.encoder = LaBSEOnnxEncoder()

        # åˆå§‹åŒ–åˆ†å¥å™¨ï¼ˆæ”¯æŒå¤šè¯­è¨€è‡ªåŠ¨æ£€æµ‹ï¼‰
        self.text_splitter = TextSplitter(auto_detect=auto_detect_language)

        print(f"âœ“ ç¿»è¯‘è´¨é‡æ£€æŸ¥å·¥å…·åˆå§‹åŒ–å®Œæˆ")
        print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        print(f"  æœ€å¤§å¯¹é½æ•°: {max_align}")
        print(f"  åˆ†æ•°é˜ˆå€¼: {score_threshold}")
        print(f"  è·³è¿‡æƒ©ç½š: {skip} (è¶Šè´Ÿè¶Šå€¾å‘N:Må¯¹é½)")
    
    def check_translation(self, source_text, target_text, is_split=True,
                         source_language='auto', target_language='auto'):
        """
        æ£€æŸ¥ç¿»è¯‘è´¨é‡

        å‚æ•°:
            source_text: åŸæ–‡æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²æˆ–å¥å­åˆ—è¡¨ï¼‰
            target_text: è¯‘æ–‡æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²æˆ–å¥å­åˆ—è¡¨ï¼‰
            is_split: æ˜¯å¦å·²ç»åˆ†å¥
            source_language: æºè¯­è¨€ ('en', 'zh', 'auto')
            target_language: ç›®æ ‡è¯­è¨€ ('en', 'zh', 'auto')

        è¿”å›:
            results: æ£€æŸ¥ç»“æœå­—å…¸
        """
        print("\n" + "="*80)
        print("å¼€å§‹ç¿»è¯‘è´¨é‡æ£€æŸ¥")
        print("="*80)

        # æ­¥éª¤0: æ–‡æœ¬åˆ†å¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        detected_src_lang = None
        detected_tgt_lang = None

        if not is_split:
            print("\næ­¥éª¤0: æ–‡æœ¬åˆ†å¥...")
            if isinstance(source_text, str):
                source_sents = self.text_splitter.split_sentences(source_text, source_language)
                # ğŸ†• è·å–æ£€æµ‹åˆ°çš„è¯­è¨€ï¼ˆç”¨äºä¼ é€’ç»™ Bertalignï¼Œé¿å…è°ƒç”¨ Google Translateï¼‰
                if source_language == 'auto' and self.text_splitter.language_detector:
                    detected_src_lang = self.text_splitter.language_detector.detect(source_text)
                    print(f"  æ£€æµ‹åˆ°æºè¯­è¨€: {detected_src_lang}")
                elif source_language != 'auto':
                    detected_src_lang = source_language
                else:
                    # å¦‚æœæ˜¯ 'auto' ä½†è¯­è¨€æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€
                    detected_src_lang = 'en'
                    print(f"  âš ï¸  è¯­è¨€æ£€æµ‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æºè¯­è¨€: {detected_src_lang}")
                print(f"  æºæ–‡æœ¬åˆ†å¥: {len(source_sents)}å¥")
            else:
                source_sents = source_text

            if isinstance(target_text, str):
                target_sents = self.text_splitter.split_sentences(target_text, target_language)
                # ğŸ†• è·å–æ£€æµ‹åˆ°çš„è¯­è¨€ï¼ˆç”¨äºä¼ é€’ç»™ Bertalignï¼Œé¿å…è°ƒç”¨ Google Translateï¼‰
                if target_language == 'auto' and self.text_splitter.language_detector:
                    detected_tgt_lang = self.text_splitter.language_detector.detect(target_text)
                    print(f"  æ£€æµ‹åˆ°ç›®æ ‡è¯­è¨€: {detected_tgt_lang}")
                elif target_language != 'auto':
                    detected_tgt_lang = target_language
                else:
                    # å¦‚æœæ˜¯ 'auto' ä½†è¯­è¨€æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€
                    detected_tgt_lang = 'zh'
                    print(f"  âš ï¸  è¯­è¨€æ£€æµ‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡è¯­è¨€: {detected_tgt_lang}")
                print(f"  ç›®æ ‡æ–‡æœ¬åˆ†å¥: {len(target_sents)}å¥")
            else:
                target_sents = target_text

            # è½¬æ¢ä¸ºBertalignéœ€è¦çš„æ ¼å¼ï¼ˆæ¢è¡Œåˆ†éš”ï¼‰
            source_text_for_align = '\n'.join(source_sents)
            target_text_for_align = '\n'.join(target_sents)
            is_split = True
        else:
            source_text_for_align = source_text
            target_text_for_align = target_text

        # æ­¥éª¤1: ä½¿ç”¨Bertalignè¿›è¡Œå¥å­å¯¹é½
        print("\næ­¥éª¤1: æ‰§è¡Œå¥å­å¯¹é½...")

        aligner = Bertalign(
            src=source_text_for_align,
            tgt=target_text_for_align,
            max_align=self.max_align,
            top_k=self.top_k,
            skip=self.skip,
            win=self.win,
            is_split=is_split,
            src_lang=detected_src_lang,  # ğŸ†• ä¼ å…¥è¯­è¨€ä»£ç ï¼Œé¿å…è°ƒç”¨ Google Translate
            tgt_lang=detected_tgt_lang   # ğŸ†• ä¼ å…¥è¯­è¨€ä»£ç ï¼Œé¿å…è°ƒç”¨ Google Translate
        )
        aligner.align_sents()
        
        src_sents = aligner.src_sents
        tgt_sents = aligner.tgt_sents
        alignments = aligner.result
        
        print(f"âœ“ å¯¹é½å®Œæˆ: {len(src_sents)}å¥æºæ–‡æœ¬ â†’ {len(tgt_sents)}å¥ç›®æ ‡æ–‡æœ¬")
        print(f"  å…±{len(alignments)}ä¸ªå¯¹é½ç»„")
        
        # æ­¥éª¤2: è®¡ç®—æ¯ä¸ªå¯¹é½ç»„çš„ç›¸ä¼¼åº¦
        print("\næ­¥éª¤2: è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦...")
        alignment_scores = []

        for src_indices, tgt_indices in alignments:
            # ğŸ”´ ä¿®å¤: å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºç©ºå¯¹é½ï¼ˆç¼ºå¤±/å¢æ·»ï¼‰
            if len(src_indices) == 0 or len(tgt_indices) == 0:
                # ç©ºå¯¹é½ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ ‡è®°ä¸ºnull
                alignment_scores.append({
                    'src_indices': [int(i) for i in src_indices],
                    'tgt_indices': [int(i) for i in tgt_indices],
                    'src_texts': [src_sents[i] for i in src_indices] if len(src_indices) > 0 else [],
                    'tgt_texts': [tgt_sents[i] for i in tgt_indices] if len(tgt_indices) > 0 else [],
                    'src_text': " ".join([src_sents[i] for i in src_indices]) if len(src_indices) > 0 else "",
                    'tgt_text': " ".join([tgt_sents[i] for i in tgt_indices]) if len(tgt_indices) > 0 else "",
                    'similarity': None,  # æ ‡è®°ä¸ºNoneè€Œé0.0
                    'is_null_alignment': True
                })
                continue

            # æå–å¥å­æ–‡æœ¬
            src_texts = [src_sents[i] for i in src_indices]
            tgt_texts = [tgt_sents[i] for i in tgt_indices]

            # ğŸ†• å¯¹äºN:Må¯¹é½ï¼Œä½¿ç”¨æœ€å°ç›¸ä¼¼åº¦ç­–ç•¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            if self.use_min_similarity and (len(src_texts) > 1 or len(tgt_texts) > 1):
                # ç¼–ç æ‰€æœ‰å¥å­
                src_embeddings = self.encoder.encode_sentences(src_texts)
                tgt_embeddings = self.encoder.encode_sentences(tgt_texts)

                # è®¡ç®—æ‰€æœ‰æº-ç›®æ ‡å¥å­å¯¹çš„ç›¸ä¼¼åº¦ï¼Œå–æœ€å°å€¼
                min_sim = 1.0
                for src_emb in src_embeddings:
                    src_emb = src_emb / np.linalg.norm(src_emb)
                    for tgt_emb in tgt_embeddings:
                        tgt_emb = tgt_emb / np.linalg.norm(tgt_emb)
                        sim = float(np.dot(src_emb, tgt_emb))
                        min_sim = min(min_sim, sim)

                similarity = min_sim
            else:
                # 1:1å¯¹é½æˆ–ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ç­–ç•¥
                # ç¼–ç æºå¥å­
                src_embeddings = self.encoder.encode_sentences(src_texts)
                src_emb = np.mean(src_embeddings, axis=0)
                src_emb = src_emb / np.linalg.norm(src_emb)

                # ç¼–ç ç›®æ ‡å¥å­
                tgt_embeddings = self.encoder.encode_sentences(tgt_texts)
                tgt_emb = np.mean(tgt_embeddings, axis=0)
                tgt_emb = tgt_emb / np.linalg.norm(tgt_emb)

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = float(np.dot(src_emb, tgt_emb))

            alignment_scores.append({
                'src_indices': [int(i) for i in src_indices],
                'tgt_indices': [int(i) for i in tgt_indices],
                'src_texts': src_texts,  # ä¿ç•™å•ç‹¬çš„å¥å­åˆ—è¡¨
                'tgt_texts': tgt_texts,  # ä¿ç•™å•ç‹¬çš„å¥å­åˆ—è¡¨
                'src_text': " ".join(src_texts),  # åˆå¹¶æ–‡æœ¬ç”¨äºæ˜¾ç¤º
                'tgt_text': " ".join(tgt_texts),  # åˆå¹¶æ–‡æœ¬ç”¨äºæ˜¾ç¤º
                'similarity': similarity,
                'is_null_alignment': False
            })

        print(f"âœ“ ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")

        # æ­¥éª¤2.5: è‡ªåŠ¨æ‹†æ•£N:Må¯¹é½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.auto_split_nm:
            print("\næ­¥éª¤2.5: æ£€æŸ¥æ˜¯å¦éœ€è¦æ‹†æ•£N:Må¯¹é½...")
            new_alignment_scores = []
            split_count = 0

            for item in alignment_scores:
                if item.get('is_null_alignment', False):
                    new_alignment_scores.append(item)
                    continue

                src_indices = item['src_indices']
                tgt_indices = item['tgt_indices']

                # åªå¤„ç†N:Nå¯¹é½ï¼ˆN==Mä¸”N>1ï¼‰
                if len(src_indices) == len(tgt_indices) and len(src_indices) > 1:
                    # è®¡ç®—æ‹†æ•£åçš„1:1ç›¸ä¼¼åº¦
                    individual_sims = []
                    for i in range(len(src_indices)):
                        src_emb = self.encoder.encode_sentences([item['src_texts'][i]])[0]
                        tgt_emb = self.encoder.encode_sentences([item['tgt_texts'][i]])[0]
                        src_emb = src_emb / np.linalg.norm(src_emb)
                        tgt_emb = tgt_emb / np.linalg.norm(tgt_emb)
                        sim = float(np.dot(src_emb, tgt_emb))
                        individual_sims.append(sim)

                    # å¦‚æœæ‰€æœ‰1:1ç›¸ä¼¼åº¦éƒ½é«˜äºN:Nç›¸ä¼¼åº¦ï¼Œåˆ™æ‹†æ•£
                    avg_individual_sim = np.mean(individual_sims)
                    if avg_individual_sim > item['similarity']:
                        # æ‹†æ•£ä¸ºå¤šä¸ª1:1å¯¹é½
                        for i in range(len(src_indices)):
                            new_alignment_scores.append({
                                'src_indices': [src_indices[i]],
                                'tgt_indices': [tgt_indices[i]],
                                'src_texts': [item['src_texts'][i]],
                                'tgt_texts': [item['tgt_texts'][i]],
                                'src_text': item['src_texts'][i],
                                'tgt_text': item['tgt_texts'][i],
                                'similarity': individual_sims[i],
                                'is_null_alignment': False
                            })
                        split_count += 1
                    else:
                        new_alignment_scores.append(item)
                else:
                    new_alignment_scores.append(item)

            alignment_scores = new_alignment_scores
            if split_count > 0:
                print(f"âœ“ æ‹†æ•£äº† {split_count} ä¸ªN:Må¯¹é½")

        # æ­¥éª¤3: æ£€æµ‹å¼‚å¸¸
        print("\næ­¥éª¤3: æ£€æµ‹ç¿»è¯‘å¼‚å¸¸...")

        # ğŸ”´ ä¿®å¤: å…ˆä»ç©ºå¯¹é½ä¸­æå–ç¼ºå¤±/å¢æ·»
        omissions = []
        additions = []
        low_similarity = []

        # ğŸ†• å¼ºåˆ¶æ‹†æ•£çš„å¯¹é½ç»„ï¼ˆç”¨äºåç»­è¡¥å……ç¼ºå¤±/å¢æ·»ï¼‰
        force_split_alignments = []

        for item in alignment_scores:
            if item.get('is_null_alignment', False):
                # ç©ºå¯¹é½ï¼šåˆ¤æ–­æ˜¯ç¼ºå¤±è¿˜æ˜¯å¢æ·»
                if len(item['src_indices']) > 0 and len(item['tgt_indices']) == 0:
                    # ç¼ºå¤±ï¼šæœ‰æºæ— ç›®æ ‡
                    for idx in item['src_indices']:
                        omissions.append({
                            'type': 'omission',
                            'src_index': idx,
                            'src_text': src_sents[idx]
                        })
                elif len(item['src_indices']) == 0 and len(item['tgt_indices']) > 0:
                    # å¢æ·»ï¼šæ— æºæœ‰ç›®æ ‡
                    for idx in item['tgt_indices']:
                        additions.append({
                            'type': 'addition',
                            'tgt_index': idx,
                            'tgt_text': tgt_sents[idx]
                        })
            else:
                # ğŸ†• äº‹åæ¸…æ´—ï¼šå¼ºåˆ¶æ‹†æ•£ä½ç›¸ä¼¼åº¦å¯¹é½ç»„
                if item['similarity'] < self.force_split_threshold:
                    # ç›¸ä¼¼åº¦æä½ï¼Œå¼ºåˆ¶æ‹†æ•£ä¸ºç¼ºå¤±+å¢æ·»
                    for idx in item['src_indices']:
                        omissions.append({
                            'type': 'omission',
                            'src_index': idx,
                            'src_text': src_sents[idx]
                        })
                    for idx in item['tgt_indices']:
                        additions.append({
                            'type': 'addition',
                            'tgt_index': idx,
                            'tgt_text': tgt_sents[idx]
                        })
                    # è®°å½•è¢«æ‹†æ•£çš„å¯¹é½ç»„
                    force_split_alignments.append(item)
                # æœ‰æ•ˆå¯¹é½ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦
                elif item['similarity'] < self.similarity_threshold:
                    low_similarity.append({
                        'type': 'low_similarity',
                        'src_indices': item['src_indices'],
                        'tgt_indices': item['tgt_indices'],
                        'src_text': item['src_text'],
                        'tgt_text': item['tgt_text'],
                        'similarity': item['similarity']
                    })

        # 3.1 æ£€æµ‹æœªè¢«ä»»ä½•å¯¹é½è¦†ç›–çš„å¥å­ï¼ˆè¡¥å……æ£€æŸ¥ï¼‰
        # ğŸ†• æ’é™¤è¢«å¼ºåˆ¶æ‹†æ•£çš„å¯¹é½ç»„
        aligned_src_indices = set()
        aligned_tgt_indices = set()

        # æ”¶é›†å·²ç»æ·»åŠ åˆ°ç¼ºå¤±/å¢æ·»çš„ç´¢å¼•ï¼ˆæ¥è‡ªå¼ºåˆ¶æ‹†æ•£ï¼‰
        existing_omission_indices = set(item['src_index'] for item in omissions)
        existing_addition_indices = set(item['tgt_index'] for item in additions)

        for item in alignment_scores:
            if item in force_split_alignments:
                # è¢«æ‹†æ•£çš„å¯¹é½ç»„ï¼Œå·²ç»åœ¨å‰é¢å¤„ç†ä¸ºç¼ºå¤±+å¢æ·»ï¼Œä¸éœ€è¦å†å¤„ç†
                pass
            else:
                aligned_src_indices.update(item['src_indices'])
                aligned_tgt_indices.update(item['tgt_indices'])

        # è¡¥å……ç¼ºå¤±ï¼ˆæ’é™¤å·²ç»æ·»åŠ è¿‡çš„ï¼‰
        for i in range(len(src_sents)):
            if i not in aligned_src_indices and i not in existing_omission_indices:
                omissions.append({
                    'type': 'omission',
                    'src_index': i,
                    'src_text': src_sents[i]
                })

        # è¡¥å……å¢æ·»ï¼ˆæ’é™¤å·²ç»æ·»åŠ è¿‡çš„ï¼‰
        for i in range(len(tgt_sents)):
            if i not in aligned_tgt_indices and i not in existing_addition_indices:
                additions.append({
                    'type': 'addition',
                    'tgt_index': i,
                    'tgt_text': tgt_sents[i]
                })
        
        print(f"âœ“ å¼‚å¸¸æ£€æµ‹å®Œæˆ:")
        print(f"  ç¼ºå¤± (Omission): {len(omissions)}å¤„")
        print(f"  å¢æ·» (Addition): {len(additions)}å¤„")
        print(f"  ç›¸ä¼¼åº¦ä½ (Low Similarity): {len(low_similarity)}å¤„")
        if force_split_alignments:
            print(f"  å¼ºåˆ¶æ‹†æ•£å¯¹é½ç»„: {len(force_split_alignments)}ä¸ª (ç›¸ä¼¼åº¦ < {self.force_split_threshold})")
        
        # æ±‡æ€»ç»“æœ
        results = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'source_sentences': len(src_sents),
                'target_sentences': len(tgt_sents),
                'alignments': len(alignments),
                'similarity_threshold': self.similarity_threshold,
                'force_split_threshold': self.force_split_threshold
            },
            'alignments': alignment_scores,
            'force_split_alignments': force_split_alignments,  # ğŸ†• è®°å½•è¢«æ‹†æ•£çš„å¯¹é½ç»„
            'issues': {
                'omissions': omissions,
                'additions': additions,
                'low_similarity': low_similarity
            },
            'summary': {
                'total_issues': len(omissions) + len(additions) + len(low_similarity),
                'omission_count': len(omissions),
                'addition_count': len(additions),
                'low_similarity_count': len(low_similarity),
                'force_split_count': len(force_split_alignments)  # ğŸ†•
            }
        }

        return results

    def save_report_json(self, results, output_path="translation_qa_report.json"):
        """
        ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š

        å‚æ•°:
            results: check_translation()è¿”å›çš„ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    def save_report_csv(self, results, output_path="translation_qa_report.csv"):
        """
        ä¿å­˜CSVæ ¼å¼æŠ¥å‘Šï¼ˆæŒ‰è¦æ±‚çš„å¤šè¡Œå¹³é“ºæ ¼å¼ï¼‰

        æ ¼å¼è¯´æ˜ï¼š
        - 1:Nå¯¹é½ï¼šæºæ–‡æœ¬åªåœ¨ç¬¬ä¸€è¡Œæ˜¾ç¤ºï¼Œåç»­è¡Œç•™ç©ºï¼›ç›®æ ‡æ–‡æœ¬æ¯è¡Œæ˜¾ç¤ºä¸€å¥
        - N:1å¯¹é½ï¼šç›®æ ‡æ–‡æœ¬åªåœ¨ç¬¬ä¸€è¡Œæ˜¾ç¤ºï¼Œåç»­è¡Œç•™ç©ºï¼›æºæ–‡æœ¬æ¯è¡Œæ˜¾ç¤ºä¸€å¥
        - N:Må¯¹é½ï¼šæŒ‰max(N,M)å±•å¼€ï¼Œè¶…å‡ºéƒ¨åˆ†ç•™ç©º

        å‚æ•°:
            results: check_translation()è¿”å›çš„ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # ğŸ”´ ä¿®å¤: å°†æ‰€æœ‰è¡Œåˆå¹¶åˆ°ä¸€ä¸ªåˆ—è¡¨ï¼Œç„¶åæŒ‰æºç´¢å¼•æ’åº
        all_rows = []

        # ğŸ†• è·å–è¢«æ‹†æ•£çš„å¯¹é½ç»„åˆ—è¡¨
        force_split_set = set()
        for fs_item in results.get('force_split_alignments', []):
            # ä½¿ç”¨å¯¹é½ç»„çš„ç´¢å¼•æ ‡è¯†ï¼ˆé€šè¿‡src_indiceså’Œtgt_indicesçš„å…ƒç»„ï¼‰
            force_split_set.add((tuple(fs_item['src_indices']), tuple(fs_item['tgt_indices'])))

        # å¯¹é½ç»„ - æŒ‰è¦æ±‚çš„æ ¼å¼å¹³é“º
        for alignment_idx, item in enumerate(results['alignments']):
            src_indices = item['src_indices']
            tgt_indices = item['tgt_indices']
            src_texts = item.get('src_texts', [item['src_text']])
            tgt_texts = item.get('tgt_texts', [item['tgt_text']])
            similarity = item['similarity']

            # ğŸ†• è·³è¿‡è¢«æ‹†æ•£çš„å¯¹é½ç»„
            if (tuple(src_indices), tuple(tgt_indices)) in force_split_set:
                continue

            # åˆ¤æ–­å¼‚å¸¸ç±»å‹
            if similarity is None:
                # ç©ºå¯¹é½ï¼Œå·²åœ¨Step 3ä¸­å¤„ç†ä¸ºç¼ºå¤±/å¢æ·»
                continue
            elif similarity < self.similarity_threshold:
                exception_type = 'ç›¸ä¼¼åº¦ä½ (Low Similarity)'
            else:
                exception_type = 'OK'

            # è®¡ç®—éœ€è¦çš„è¡Œæ•°
            max_rows = max(len(src_indices), len(tgt_indices))

            # ğŸ”´ ä¿®å¤: ä½¿ç”¨ç¬¬ä¸€ä¸ªæºç´¢å¼•ä½œä¸ºæ’åºé”®ï¼ˆå¦‚æœæœ‰æºç´¢å¼•çš„è¯ï¼‰
            # å¯¹äºN:Må¯¹é½ï¼Œæ‰€æœ‰è¡Œéƒ½åº”è¯¥ä½¿ç”¨ç›¸åŒçš„æ’åºé”®ï¼Œè¿™æ ·å®ƒä»¬ä¼šè¢«æ’åœ¨ä¸€èµ·
            if len(src_indices) > 0:
                sort_key = src_indices[0]
            elif len(tgt_indices) > 0:
                # å¦‚æœæ²¡æœ‰æºç´¢å¼•ï¼ˆå¢æ·»ï¼‰ï¼Œä½¿ç”¨ç›®æ ‡ç´¢å¼• + å¤§åç§»é‡
                sort_key = 999999 + tgt_indices[0]
            else:
                sort_key = 999999

            # æŒ‰è¦æ±‚çš„é€»è¾‘å¹³é“º
            for row_idx in range(max_rows):
                # ç¡®å®šå½“å‰è¡Œæ˜¾ç¤ºçš„æºæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬
                if row_idx < len(src_texts):
                    src_text = src_texts[row_idx]
                    src_idx = src_indices[row_idx]
                else:
                    src_text = ''
                    src_idx = ''

                if row_idx < len(tgt_texts):
                    tgt_text = tgt_texts[row_idx]
                    tgt_idx = tgt_indices[row_idx]
                else:
                    tgt_text = ''
                    tgt_idx = ''

                # ç¬¬ä¸€è¡Œæ˜¾ç¤ºç›¸ä¼¼åº¦å’Œå¼‚å¸¸æƒ…å†µ
                if row_idx == 0:
                    show_similarity = f"{similarity:.4f}"
                    show_exception = exception_type
                else:
                    show_similarity = ''
                    show_exception = exception_type if exception_type != 'OK' else ''

                # ğŸ”´ ä¿®å¤: ä¸ºäº†ä¿æŒN:Må¯¹é½çš„å¤šè¡Œåœ¨ä¸€èµ·ï¼Œä½¿ç”¨å­æ’åºé”®
                # sort_keyç›¸åŒæ—¶ï¼ŒæŒ‰row_idxæ’åº
                subsort_key = sort_key + (row_idx * 0.001)  # æ·»åŠ å°æ•°éƒ¨åˆ†æ¥ä¿æŒé¡ºåº

                all_rows.append({
                    'åŸæ–‡ (Source)': src_text,
                    'è¯‘æ–‡ (Target)': tgt_text,
                    'æºç´¢å¼•': src_idx,
                    'ç›®æ ‡ç´¢å¼•': tgt_idx,
                    'ç›¸ä¼¼åº¦ (Similarity)': show_similarity,
                    'å¼‚å¸¸æƒ…å†µ (Exception)': show_exception,
                    '_sort_key': subsort_key  # ä½¿ç”¨å­æ’åºé”®
                })

        # ç¼ºå¤± (Omission)
        for item in results['issues']['omissions']:
            all_rows.append({
                'åŸæ–‡ (Source)': item['src_text'],
                'è¯‘æ–‡ (Target)': '',
                'æºç´¢å¼•': item['src_index'],
                'ç›®æ ‡ç´¢å¼•': '',
                'ç›¸ä¼¼åº¦ (Similarity)': '',
                'å¼‚å¸¸æƒ…å†µ (Exception)': 'ç¼ºå¤± (Omission)',
                '_sort_key': item['src_index']
            })

        # å¢æ·» (Addition)
        for item in results['issues']['additions']:
            all_rows.append({
                'åŸæ–‡ (Source)': '',
                'è¯‘æ–‡ (Target)': item['tgt_text'],
                'æºç´¢å¼•': '',
                'ç›®æ ‡ç´¢å¼•': item['tgt_index'],
                'ç›¸ä¼¼åº¦ (Similarity)': '',
                'å¼‚å¸¸æƒ…å†µ (Exception)': 'å¢æ·» (Addition)',
                '_sort_key': 999999 + item['tgt_index']  # å¢æ·»æ’åœ¨æœ€å
            })

        # ğŸ”´ ä¿®å¤: æŒ‰æºç´¢å¼•æ’åº
        all_rows.sort(key=lambda x: x['_sort_key'])

        # ç§»é™¤æ’åºé”®
        for row in all_rows:
            del row['_sort_key']

        df = pd.DataFrame(all_rows)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')

        print(f"âœ“ CSVæŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    def print_summary(self, results):
        """
        æ‰“å°æ£€æŸ¥ç»“æœæ‘˜è¦

        å‚æ•°:
            results: check_translation()è¿”å›çš„ç»“æœ
        """
        print("\n" + "="*80)
        print("ç¿»è¯‘è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
        print("="*80)

        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æºæ–‡æœ¬å¥å­æ•°: {results['metadata']['source_sentences']}")
        print(f"  ç›®æ ‡æ–‡æœ¬å¥å­æ•°: {results['metadata']['target_sentences']}")
        print(f"  å¯¹é½ç»„æ•°: {results['metadata']['alignments']}")
        print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {results['metadata']['similarity_threshold']}")

        print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        print(f"  æ€»è®¡: {results['summary']['total_issues']}å¤„")
        print(f"  - ç¼ºå¤± (Omission): {results['summary']['omission_count']}å¤„")
        print(f"  - å¢æ·» (Addition): {results['summary']['addition_count']}å¤„")
        print(f"  - ç›¸ä¼¼åº¦ä½ (Low Similarity): {results['summary']['low_similarity_count']}å¤„")

        # è¯¦ç»†åˆ—å‡ºé—®é¢˜
        if results['issues']['omissions']:
            print(f"\nâŒ ç¼ºå¤± (Omission):")
            for item in results['issues']['omissions']:
                print(f"  æºå¥å­[{item['src_index']}]: {item['src_text'][:60]}...")

        if results['issues']['additions']:
            print(f"\nâ• å¢æ·» (Addition):")
            for item in results['issues']['additions']:
                print(f"  ç›®æ ‡å¥å­[{item['tgt_index']}]: {item['tgt_text'][:60]}...")

        if results['issues']['low_similarity']:
            print(f"\nâš ï¸  ç›¸ä¼¼åº¦ä½ (Low Similarity < {self.similarity_threshold}):")
            for item in results['issues']['low_similarity']:
                print(f"  ç›¸ä¼¼åº¦: {item['similarity']:.4f}")
                print(f"    æº: {item['src_text'][:60]}...")
                print(f"    è¯‘: {item['tgt_text'][:60]}...")

        print("\n" + "="*80)

